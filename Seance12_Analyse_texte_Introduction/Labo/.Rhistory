knitr::include_graphics("../Images/comparaison.png")
library(tidyverse)
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
trumptweets <- readRDS("../Data/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:6])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 5, control = list(seed = 3425))
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
View(tt_topics)
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
library(topicmodels)
data("AssociatedPress")
AssociatedPress
inspect(AssociatedPress[6:20, 10:25])
ap_lda <- LDA(AssociatedPress, k = 5, control = list(seed = 12345))
ap_lda <- LDA(AssociatedPress, k = 5, control = list(seed = 12345))
ap_lda
knitr::include_graphics("../Images/lda2.png")
knitr::include_graphics("../Images/gibbs1.png")
knitr::include_graphics("../Images/gibbs6.png")
knitr::include_graphics("../Images/gibbs5.png")
knitr::include_graphics("../Images/gibbs7.png")
knitr::include_graphics("../Images/gibbs8.png")
knitr::include_graphics("../Images/gibbs9.png")
knitr::include_graphics("../Images/gibbs10.png")
knitr::include_graphics("../Images/gibbs11.png")
knitr::include_graphics("../Images/gibbs13.png")
knitr::include_graphics("../Images/gibbs12.png")
knitr::include_graphics("../Images/gibbs6.png")
knitr::include_graphics("../Images/gibbs7.png")
knitr::include_graphics("../Images/gibbs8.png")
knitr::include_graphics("../Images/gibbs9.png")
knitr::include_graphics("../Images/gibbs10.png")
knitr::include_graphics("../Images/gibbs11.png")
knitr::include_graphics("../Images/gibbs12.png")
knitr::include_graphics("../Images/gibbs13.png")
knitr::include_graphics("../Images/gibbs14.png")
# Effacer l'environnement
rm(list = ls())
library(tidyverse)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Data/trumptweets.Rdata")
View(trumptweets)
head(trumptweets)
trumptweets$text[2]
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
View(trump_corpus)
View((trump_corpus))
# Les informations de chaque document
trump_corpus[[1]][["content"]]
trump_corpus[[2]][["meta"]]
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
# Les informations de chaque document
trump_corpus[[1]][["content"]]
View(trumptweets)
# Les informations de chaque document
trump_corpus[[7]][["content"]]
# Les informations de chaque document
trump_corpus[[8]][["content"]]
# Les informations de chaque document
trump_corpus[[3]][["content"]]
View(trumptweets)
# Les informations de chaque document
trump_corpus[[97]][["content"]]
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
# Les informations de chaque document
trump_corpus[[97]][["content"]]
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
trump_corpus[[97]][["content"]]
# Transformer en minuscule
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower))
trump_corpus[[97]][["content"]]
# Enlever les espaces
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
trump_corpus[[97]][["content"]]
# Stemming
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
trump_corpus[[97]][["content"]]
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(3, Inf)))
View(trump_DTM)
inspect(trump_DTM[24:42,24:50])
View(trumptweets)
trump_DTM_matrix <- as.matrix(trump_DTM)
trump_DTM_matrix[1:5,1:8]
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text)
View(tidy_trump_tweets)
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
View(tidy_trump_tweets)
tidy_trump_tweets
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
data("stop_words")
head(stopwords(), 24)
head(fruits)
head(fruit)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
http_trump <- data.frame(word = c("https", "t.co", "amp", "rt"))
http_trump
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(http_trump)
tidy_trump_tweets  %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets <- tidy_trump_tweets %>%
mutate_at("word", funs(wordStem((.), language="en")))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
View(tidy_trump_tweets)
tidy_trump_DTM <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(tidy_trump_DTM[1:5, 1:8])
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
View(top_20)
top_20 <- top_20[1:20, ]
top_20
View(top_20)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Nombre de fois qu'un mot apparaît dans les tweets") +
xlab("Mot") +
guides(fill = FALSE)
tidy_trump_tweets_tfidf <-
tidy_trump_tweets %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
View(tidy_trump_tweets_tfidf)
top_tfidf <-
tidy_trump_tweets_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n) %>%
filter(tf_idf < 4)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
View(tidy_trump_tfidf)
dtm_trumptweets <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(dtm_trumptweets[1:5,1:8])
dtm_trumptweets_matrix <- as.matrix(dtm_trumptweets)
dtm_trumptweets_matrix[1:5,1:8]
dtm_trumptweets_matrix %>% {
wordcloud(.$word, .$n, max.words = 20)
}
wordcloud(.word, .n, max.words = 20)
dtm_trumptweets_matrix %>% {
wordcloud(.word, .n, max.words = 20)
}
?wordcloud
wordcloud(dtm_trumptweets)
dtm_trumptweets %>% {
wordcloud(.$word, .$n, max.words = 20)
}
wordcloud(dtm_trumptweets)
View(tidy_trump_tweets)
View(tidy_trump_tfidf)
wordcloud(tidy_trump_tfidf$word, tidy_trump_tfidf$n)
wordcloud(tidy_trump_tfidf$word, tidy_trump_tfidf$n, max.words = 20)
wordcloud(tidy_trump_tfidf$word, tidy_trump_tfidf$n, max.words = 40)
# Effacer l'environnement
rm(list = ls())
# Installer les différents packages
# Charger le packages
library(tidyverse)
library(lubridate)
library(stringr)
library(forcats)
library(modelr)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
View(corpus_raw)
rm(list = ls())
library(tidyverse)
library(tidytext)
library(textdata) #
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Data/trumptweets.RData")
View(trumptweets)
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text)
View(tidy_trump_tweets)
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
View(tidy_trump_tweets)
head(tidy_trump_tweets, 12)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("https|t.co|amp|rt", word))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
top_20 <- top_20[1:20, ]
top_20
ggplot(top_20) +
geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
theme_minimal() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
labs(x = "Mot", y = "Nombre de fois que le mot apparait dans un tweet") +
guides(fill = FALSE)
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(n))
top_tfidf
head(top_tfidf$word)
top_tfidf <-
tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
View(tidy_trump_tweets)
economic_dictionary <- "economy|unemployment|trade|tariffs|employment|tax"
economic_dictionary1 <- "Togo|Cameroon"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
head(economic_tweets$text)
head(get_sentiments("afinn"), 24)
summary(get_sentiments("afinn"))
head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))
head(get_sentiments("nrc"), 24)
summary(get_sentiments("nrc"))
dictionnaire <- get_sentiments("bing")
View(dictionnaire)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
View(trump_tweet_sentiment)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire)
View(trump_tweet_sentiment)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
ggplot(trump_tweet_sentiment) +
geom_line(aes(x=created_at, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
View(trump_tweet_sentiment)
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(date = date(created_at))
View(tidy_trump_tweets)
trump_tweet_sentiment1 <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
View(trump_tweet_sentiment1)
ggplot(trump_tweet_sentiment1) +
geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
trump_sentiment_negatif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="negative") %>%
count(date, sentiment)
trump_sentiment_negatif
trump_sentiment_positif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="positive") %>%
count(date, sentiment)
trump_sentiment <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
trump_sentiment
negatif <-
ggplot(trump_sentiment_negatif) +
geom_line(aes(x = date, y = n), color = "red") +
labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")
negatif
trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")
View(trump_approval)
trump_approval <-
trump_approval %>%
mutate(date = mdy(modeldate))
approval_plot <-
trump_approval %>%
filter(subgroup == "Adults") %>%
#filter(date > min(trump_sentiment_plot$date)) %>%
group_by(date) %>%
summarise(approval = mean(approve_estimate))
approval <-
ggplot(approval_plot) +
geom_line(aes(x = date, y = approval)) +
#theme_minimal()+
labs(x = "Date", y = "% des Américains qui aprouvent Trump")
approval
library(ggpubr)
ggarrange(negatif, approval, nrow = 2)
nrc <- get_sentiments("nrc")
nrc
trump_tweet_sentiment_nrc <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
count(date, sentiment)
trump_tweet_sentiment_nrc
tidy_trump_tweets_nrc1 <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
group_by(sentiment) %>%
count() %>%
ungroup()%>%
arrange(desc(sentiment)) %>%
mutate(percentage = round(n/sum(n), 4)*100,
lab.pos = cumsum(percentage)-.5*percentage)
ggplot(data = tidy_trump_tweets_nrc1, aes(x = 2, y = percentage, fill = sentiment))+
geom_bar(stat = "identity")+
coord_polar("y", start = 200) +
geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
theme_void() +
#scale_fill_brewer(palette = "Dark2")+
xlim(.5, 2.5) +
ggtitle("Sentiment dans les tweets de Trump")
