library(tm)
library(SnowballC)
library(tidytext)
library(tidytext)
library(wordcloud)
library(qss)
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
corpus_raw[[1]][1]
View(corpus_raw)
corpus_tidy <- tidy(corpus_raw, "corpus")
View(corpus_tidy)
corpus_tidy <- corpus_tidy %>%
mutate(id_num1 = str_sub(id, 3, 4),
id_num = as.integer(str_extract(id, pattern = "\\d+")))
class(corpus_tidy$id_num)
corpus_tidy_text <- corpus_tidy %>%
select(id_num, text) %>%
unnest_tokens("word", text)
corpus_tidy_text <-
corpus_tidy_text %>%
anti_join(stop_words)
corpus_tidy_text <-
corpus_tidy_text %>%
filter(!grepl("\\b\\d+\\b", word))
corpus_tidy_text <-
corpus_tidy_text %>%
mutate(word = gsub("\\s", "", word))
corpus_tidy_text <-
corpus_tidy_text %>%
mutate_at("word", funs(wordStem((.), language = "en")))
corpus_dtm
View(corpus_tidy_text)
corpus_dtm <-
corpus_tidy_text %>%
count(id_num, word) %>%
cast_dtm(id_num, word, n)
inspect(corpus_dtm[1:5,1:6])
View(corpus_tidy_text)
graph_doc <- function(donnee){
ggplot(donnee) +
geom_col(aes(x = reorder(word, desc(n)), y = n, fill = word), show.legend = FALSE) +
labs(x = "Word") +
coord_flip() #+
#guides(color = FALSE)
}
doc12 <- corpus_tidy_text %>%
filter(id_num == 12) %>%
group_by(word) %>%
count() %>%
arrange(desc(n))
View(doc12)
doc12_10 <- doc12[1:10, ]
graph_doc(doc12_10)
# Nuage document 24
graph_doc(doc24_10)
corpus_tidy_text_tfidf <- corpus_tidy_text %>%
count(word, id_num) %>%
bind_tf_idf(word, id_num, n)
View(corpus_tidy_text_tfidf)
doc12_idf <-
corpus_tidy_text_tfidf %>%
filter(id_num == 12) %>%
arrange(desc(tf_idf))
doc12_idf_10 <- doc12_idf[1:10, ]
graph_doc(doc12_idf_10)
graph_doc(doc12_10)
filter(corpus_tidy_text_tfidf, id_num == 12) %>% {
wordcloud(.$word, .$n, max.words = 20)
}
View(doc12_idf)
filter(corpus_tidy_text_tfidf, id_num == 12) %>% {
wordcloud(.$word, .$n, max.words = 20)
}
View(doc12_idf)
wordcloud(.$word, .$n, max.words = 20)
filter(doc12_idf_10, id_num == 12) %>% {
wordcloud(.$word, .$n, max.words = 20)
}
filter(corpus_tidy_text_tfidf, id_num == 12) %>% {
wordcloud(.$word, .$tf_idf, max.words = 20)
}
Maintenant, si on voulait faire les mêmes choses mais avec le wordcloud, on doit transformer la base de données en matrice document-terms. (PAS forcément).
hm <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
corpus_tidy_text_tfidf <-
corpus_tidy_text_tfidf %>%
mutate(Author = if_else(id_num %in% hm, "Hamilton", "NA"))
Hamilton <- corpus_tidy_text_tfidf %>%
filter(Author == "Hamilton")
View(Hamilton)
Hamilton_dtm <- Hamilton %>%
cast_dtm(id_num, word, tf_idf)
inspect(Hamilton_dtm[7:10, 2:7])
CLUSTERS <- 4
kmean_out <-
Hamilton_dtm %>%
kmeans(centers = CLUSTERS, nstart = 10)
View(kmean_out)
kmean_out$iter
hamilton_words <-
tibble(word = colnames(Hamilton_dtm))
View(hamilton_words)
hamilton_words <- bind_cols(hamilton_words, as_tibble(t(kmean_out$centers)))
View(hamilton_words)
top_word <-
hamilton_words %>%
pivot_longer(cols = c("1":"4"), "cluster", "value") %>%
group_by(cluster) %>%
top_n(10, value)
ggplot(top_word %>% filter(cluster == 2)) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
ggplot(top_word %>% filter(cluster == 1)) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
facet_wrap(~cluster)
ggplot(top_word ) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster)
top_word_summary <-
top_word %>%
group_by(cluster) %>%
summarise(top_word = str_c(word, collapse = ", "))
top_word_summary
enframe(kmean_out$cluster, "id_num", "cluster") %>%
group_by(cluster) %>%
summarise(documents = str_c(id_num, collapse = ", "))
library(topicmodels)
Hamilton_dtm_tf <- Hamilton %>%
cast_dtm(id_num, word, n)
inspect(Hamilton_dtm[1:5, 2:6])
topic_hamilton_lda <- LDA(Hamilton_dtm_tf, k = 4, control = list(seed = 2342))
topic_hamilton_data <- tidy(topic_hamilton_lda, matrix = "beta")
topic_hamilton_data
tt_top_term <-
topic_hamilton_data %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
filter(term != "corpus") %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, asc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value, color = cluster)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value, fill = cluster)) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value, fill = cluster)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value, fill = cluster)) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
ggplot(top_word ) +
geom_col(aes(x = reorder(word, -desc(value)), y = value, fill = cluster), show.legend = FALSE) +
coord_flip() +
facet_wrap(~cluster, scales = "free")
hm <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
ma <- c(10, 14, 37:48, 58)
ja <- c(2:5, 64)
HAMILTON_ESSAYS <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
MADISON_ESSAYS <- c(10, 14, 37:48, 58)
JAY_ESSAYS <- c(2:5, 64)
known_essays <- bind_rows(tibble(id_num = MADISON_ESSAYS,
author = "Madison"),
tibble(id_num = HAMILTON_ESSAYS,
author = "Hamilton"),
tibble(id_num = JAY_ESSAYS,
author = "Jay"))
View(known_essays)
Style_words <-
tibble(word = c("although", "always", "commonly", "consequently",
"considerable", "enough", "there", "upon", "while", "whilst"))
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text)
View(corpus_tidy_short_t)
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
group_by(id_num) %>%
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
group_by(id_num) %>%
mutate(prop = n / sum(n) * 1000)
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
group_by(id_num) %>%
mutate(prop = n / sum(n) * 1000)
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
inner_join(Style_words, by = "word")
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
mutate(author = case_when(
id_num %in% hm ~ "Hamilton",
id_num %in% ma ~ "Madison",
id_num %in% ja ~ "Jay"))
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
left_join(known_essays, by = "id_num")
mots_auteurs <-
corpus_tidy_short_t %>%
filter(!is.na(author)) %>%
group_by(author, word) %>%
summarise(moyenne = mean(prop))
mots_auteurs <-
corpus_tidy_short_t %>%
filter(!is.na(author)) %>%
group_by(author, word) %>%
summarise(moyenne = mean(prop))
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
inner_join(Style_words, by = "word")
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
mutate(author = case_when(
id_num %in% hm ~ "Hamilton",
id_num %in% ma ~ "Madison",
id_num %in% ja ~ "Jay"))
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
group_by(id_num) %>%
mutate(prop = n / sum(n) * 1000)
corpus_tidy_short_t <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
group_by(id_num) %>%
mutate(prop = n / sum(n) * 1000)
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
inner_join(Style_words, by = "word")
corpus_tidy_short_t <-
corpus_tidy_short_t %>%
mutate(author = case_when(
id_num %in% hm ~ "Hamilton",
id_num %in% ma ~ "Madison",
id_num %in% ja ~ "Jay"))
mots_auteurs <-
corpus_tidy_short_t %>%
filter(!is.na(author)) %>%
group_by(author, word) %>%
summarise(moyenne = mean(prop))
mots_auteurs <-
pivot_wider(mots_auteurs, names_from = word, values_from = moyenne)
t(mots_auteurs)
freq(mots_auteurs)
View(mots_auteurs)
?t
t(mots_auteurs)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word) %>%
# term freq per 1000 words
group_by(id) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(STYLE_WORDS, by = "word") %>%
# merge known essays
left_join(known_essays, by = c("id", "document")) %>%
# make wide with each word a column
# fill empty values with 0
spread(word, count, fill = 0)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word) %>%
# term freq per 1000 words
group_by(id) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word") %>%
# merge known essays
left_join(known_essays, by = c("id", "document")) %>%
# make wide with each word a column
# fill empty values with 0
spread(word, count, fill = 0)
author_data <-
hm_tfm %>%
ungroup() %>%
filter(is.na(author) | author != "Jay") %>%
mutate(author2 = case_when(.$author == "Hamilton" ~ 1,
.$author == "Madison" ~ -1,
TRUE ~ NA_real_))
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word) %>%
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text)
View(hm_tfm)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word) %>%
# term freq per 1000 words
group_by(id) %>%
mutate(count = n / sum(n) * 1000)
View(hm_tfm)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id, word) %>%
# term freq per 1000 words
group_by(id) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word")
View(hm_tfm)
View(known_essays)
View(known_essays)
View(known_essays)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
# term freq per 1000 words
group_by(id_num) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word") %>%
# merge known essays
left_join(known_essays, by = c("id_num"))
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
# term freq per 1000 words
group_by(id_num) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word")
View(hm_tfm)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
# term freq per 1000 words
group_by(id_num) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word") %>%
# merge known essays
left_join(known_essays, by = c("id_num"))
View(hm_tfm)
hm_tfm <-
unnest_tokens(corpus_tidy, word, text) %>%
count(id_num, word) %>%
# term freq per 1000 words
group_by(id_num) %>%
mutate(count = n / sum(n) * 1000) %>%
select(-n) %>%
inner_join(Style_words, by = "word") %>%
# merge known essays
left_join(known_essays, by = c("id_num")) %>%
# make wide with each word a column
# fill empty values with 0
spread(word, count, fill = 0)
View(hm_tfm)
author_data <-
hm_tfm %>%
ungroup() %>%
filter(is.na(author) | author != "Jay") %>%
mutate(author2 = case_when(.$author == "Hamilton" ~ 1,
.$author == "Madison" ~ -1,
TRUE ~ NA_real_))
View(author_data)
hm_fit <- lm(author2 ~ upon + there + consequently + whilst,
data = author_data)
hm_fit
summary(hm_fit)
author_data <- author_data %>%
add_predictions(hm_fit)
View(author_data)
author_data <- author_data %>%
add_predictions(hm_fit) %>%
mutate(pred_author = if_else(pred >= 0, "Hamilton", "Madison"))
ctable(author_data$pred_author, author_data$author, "c")
library(summarytools)
ctable(author_data$pred_author, author_data$author, "c")
ctable(author_data$pred_author, author_data$author, "no")
author_data %>%
filter(!is.na(author)) %>%
group_by(author) %>%
summarise(`Proportion Correct` = mean(author == pred_author))
crossv_loo <- function(data, id = ".id") {
modelr::crossv_kfold(data, k = nrow(data), id = id)
}
# leave one out cross-validation object
cv <- author_data %>%
filter(!is.na(author)) %>%
crossv_loo()
View(cv)
models <- purrr::map(cv$train, ~ lm(author2 ~ upon + there + consequently + whilst,
data = ., model = FALSE))
test <- map2_df(models, cv$test,
function(mod, test) {
add_predictions(as.data.frame(test), mod) %>%
mutate(pred_author =
if_else(pred >= 0, "Hamilton", "Madison"),
correct = (pred_author == author))
})
test %>%
group_by(author) %>%
summarise(mean(correct))
author_data %>%
filter(is.na(author)) %>%
select(id_num, pred, pred_author) %>%
knitr::kable()
disputed_essays <- filter(author_data, is.na(author))$id_num
ggplot(mutate(author_data,
author = fct_explicit_na(factor(author), "Disputed")),
aes(y = id_num, x = pred, colour = author, shape = author)) +
geom_ref_line(v = 0) +
geom_point() +
scale_y_continuous(breaks = seq(10, 80, by = 10),
minor_breaks = seq(5, 80, by = 5)) +
scale_color_manual(values = c("Madison" = "blue",
"Hamilton" = "red",
"Disputed" = "black")) +
scale_shape_manual(values = c("Madison" = 16, "Hamilton" = 15,
"Disputed" = 17)) +
labs(colour = "Author", shape = "Author",
y = "Federalist Papers", x = "Predicted values")
names(kmean_out)
# Nombre d'itérations
kmean_out$iter
kmean_out$centers
kmean_out <-
Hamilton_dtm %>%
kmeans(centers = CLUSTERS, nstart = 10)
CLUSTERS <- 4
kmean_out <-
Hamilton_dtm %>%
kmeans(centers = CLUSTERS, nstart = 10)
# Éléments de la liste
names(kmean_out)
# Nombre d'itérations
kmean_out$iter
kmean_out$centers
View(kmean_out)
kmean_out$cluster
View(kmean_out)
hamilton_words <-
tibble(word = colnames(Hamilton_dtm))
View(hamilton_words)
hamilton_words <- bind_cols(hamilton_words, as_tibble(t(kmean_out$centers)))
View(hamilton_words)
top_word <-
hamilton_words %>%
pivot_longer(cols = c("1":"4"), "cluster", "value") %>%
group_by(cluster) %>%
top_n(10, value)
View(top_word)
View(known_essays)
ggplot(top_word %>% filter(cluster == 1)) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
## plotting the results using the labels and limits defined earlier
plot(Hamilton_dtm, col = kmean_out$cluster + 1, xlab = xlab, ylab = ylab,
xlim = lim, ylim = lim)
install.packages("Rgraphviz")
#install.packages("Rgraphviz")
library(Rgraphviz)
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install('Rgraphviz')
library(Rgraphviz)
## plotting the results using the labels and limits defined earlier
plot(Hamilton_dtm, col = kmean_out$cluster + 1, xlab = xlab, ylab = ylab,
xlim = lim, ylim = lim)
## plotting the results using the labels and limits defined earlier
plot(Hamilton_dtm, col = kmean_out$cluster + 1)
## plotting the centroids
points(kmean_out$centers, pch = 8, cex = 2)
## plotting the results using the labels and limits defined earlier
plot(Hamilton_dtm, col = kmean_out$cluster + 1,
xlim = lim, ylim = c(-1, 1))
knitr::include_graphics("../Images/fed1.png")
knitr::include_graphics("../Images/fed2.png")
